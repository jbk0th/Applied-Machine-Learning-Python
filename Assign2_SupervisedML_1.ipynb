{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "In this assignment you'll explore the relationship between model complexity and generalization performance, by adjusting key parameters of various supervised learning models. Part 1 of this assignment will look at regression and Part 2 will look at classification.\n",
    "\n",
    "## Part 1 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the following block to set up the variables needed for later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 15\n",
    "x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# You can use this function to help you visualize the dataset by\n",
    "# plotting a scatterplot of the data points\n",
    "# in the training and test sets.\n",
    "# def part1_scatter():\n",
    "#     %matplotlib notebook\n",
    "#     plt.figure()\n",
    "#     plt.scatter(X_train, y_train, label='training data')\n",
    "#     plt.scatter(X_test, y_test, label='test data')\n",
    "#     plt.legend(loc=4);\n",
    "    \n",
    "    \n",
    "# NOTE: Uncomment the function below to visualize the data, but be sure \n",
    "# to **re-comment it before submitting this assignment to the autograder**.   \n",
    "#part1_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_F_poly, y, random_state = 0)\n",
    "# # X_train.shape, y_train.shape\n",
    "# poly = PolynomialFeatures(degree=1)\n",
    "# test_array_poly = poly.fit_transform(test_array.reshape(-1,1))\n",
    "# test_array_poly.shape\n",
    "# #test_array.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Write a function that fits a polynomial LinearRegression model on the *training data* `X_train` for degrees 1, 3, 6, and 9. (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. `np.linspace(0,10,100)`) and store this in a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second row degree 3, the third row degree 6, and the fourth row degree 9.\n",
    "\n",
    "\n",
    "The figure above shows the fitted models plotted on top of the original data (using `plot_one()`).\n",
    "\n",
    "<br>\n",
    "*This function should return a numpy array with shape `(4, 100)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <img src=\"readonly/polynomialreg1.png\" style=\"width: 1000px;\"/> # img that belongs above \n",
    "\n",
    "def answer_one():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    res = None\n",
    "    test_array = np.linspace(0, 10, 100).reshape(-1,1)\n",
    "    # gives array of 100 equally spaced values from 0 to 10\n",
    "    deg_array = np.array([1,3,6,9])\n",
    "    for deg in deg_array:\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        X_F_poly = poly.fit_transform(x.reshape(-1,1))\n",
    "        #transform the x training point with polynomial __deg transform, \n",
    "            #expects an arry not a vector (v = (#,)) (a = (#,#))\n",
    "            #with correct array input output of the form (#target vals, # features)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_F_poly, y,\n",
    "                                                            random_state = 0)\n",
    "        #create test and train split with the x transformed polynomial data, and the y target values\n",
    "        #creating polynomail reg model, because i've transformed the x model training data into polynomial form\n",
    "        linreg = LinearRegression().fit(X_train, y_train)\n",
    "        #now have created my model with input polynomial training data\n",
    "            # do not need to reshape again b/c with upstream reshape b4 poly.fit_transform, array spit out in approp shaping\n",
    "            #this model now needs all inputs to have the same format so, my input test_array needs the poly transform as well\n",
    "            #before the linreg.predict (polynomial) can use it (FORMAT is important)\n",
    "        test_poly  = poly.fit_transform(test_array) #in the correct form fot this because the reshape done upstream\n",
    "        pred_array = linreg.predict(test_poly)\n",
    "        if res == None:\n",
    "            res = pred_array\n",
    "            #first iteration, res is None then assign its the 1st pred_array output\n",
    "        else:\n",
    "            res = np.vstack((res, pred_array))\n",
    "                #arrays to be stacked need to be input as a single object i.e tuple, list, probably just tuple\n",
    "            #once res array has the 1st element inside of it it will have appropriate shape to just stack subsequent\n",
    "            #predicted arrays on top of it\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feel free to use the function plot_one() to replicate the figure \n",
    "# from the prompt once you have completed question one\n",
    "# def plot_one(degree_predictions):\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "#     plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "#     for i,degree in enumerate([1,3,6,9]):\n",
    "#         plt.plot(np.linspace(0,10,100), degree_predictions[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "#     plt.ylim(-1,2.5)\n",
    "#     plt.legend(loc=4)\n",
    "\n",
    "# #plot_one(answer_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Write a function that fits a polynomial LinearRegression model on the training data `X_train` for degrees 0 through 9. For each model compute the $R^2$ (coefficient of determination) regression score on the training data as well as the the test data, and return both of these arrays in a tuple.\n",
    "\n",
    "*This function should return one tuple of numpy arrays `(r2_train, r2_test)`. Both arrays should have shape `(10,)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.metrics.regression import r2_score\n",
    "# deg_array = np.arange(10)\n",
    "#     #linspace wrong to use here b/c float inputs for fit_transform not plya nice and I wanted intergers anyway\n",
    "#     #arange better here the a just belies range for an array\n",
    "# r2_train = np.array([])\n",
    "# r2_test = np.array([])\n",
    "\n",
    "# for deg in deg_array:\n",
    "#         poly = PolynomialFeatures(degree=deg)\n",
    "#         x_f_poly = poly.fit_transform(x.reshape(-1,1))\n",
    "#             #reshapes input x values for acceptable input into fit_transform func, (expects array), then performs tranformation\n",
    "#                 #states reshaped array needs to be 1 col, and infers the correct row to reshape, instructions come from -1 value\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(x_f_poly, y)\n",
    "#         linreg = LinearRegression().fit(X_train, y_train)\n",
    "#             #model constructed from polynomial transformed inputs\n",
    "#         y_train_pred = linreg.predict(X_train)\n",
    "#         y_test_pred = linreg.predict(X_test)\n",
    "#         r2_train = np.append(r2_train, (r2_score(y_train, y_train_pred)))\n",
    "#         r2_test = np.append(r2_test, (r2_score(y_test, y_test_pred)))\n",
    "#     print(\"Poly Regression degree {},\\n Train Coefficient of Determination, R^2 = {:3f}\\n Test Coefficient of Determination, R^2 = {:3f}\".format(\n",
    "#                                                                    deg,\n",
    "#                                                                    r2_score(y_train, y_train_pred),\n",
    "#                                                                    r2_score(y_test, y_test_pred)))\n",
    "#             # when using the format api the : in the {} ... like{:} just signifies that anything\n",
    "#             # following the : is a outputing formatting paramter\n",
    "#             # other parameters besides precision and output type only get used after the : if\n",
    "#             # acceptable parameters are entered otherwise they're ignored\n",
    "#             # Above example {:2f} states input # represented as a float with '2' decimal precsision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.metrics.regression import r2_score\n",
    "# deg_array = np.arange(0, 10)\n",
    "#     # exclusive end\n",
    "# r2_train = np.array([])\n",
    "# r2_test = np.array([])\n",
    "# for deg in deg_array:\n",
    "#     poly = PolynomialFeatures(degree=deg)\n",
    "#     x_f_poly = poly.fit_transform(x.reshape(-1,1))\n",
    "#         #reshapes input x values for acceptable input into fit_transform func, (expects array), then performs tranformation\n",
    "#             #states reshaped array needs to be 1 col, and infers the correct row to reshape, instructions come from -1 value\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(x_f_poly, y,\n",
    "#                                                        random_state=0)\n",
    "#         #the random_state generator is impt for the same data, sets the state for the psuedo random num generator\n",
    "#     linreg = LinearRegression().fit(X_train, y_train)\n",
    "#         #model constructed from polynomial transformed inputs\n",
    "#     y_train_pred = linreg.predict(X_train)\n",
    "#     y_test_pred = linreg.predict(X_test)\n",
    "#     print('The r2_score value is {:2f}, \\nThe linreg.score value is {:2f}'.format((r2_score(y_test, y_test_pred)), (linreg.score(X_test, y_test)))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    deg_array = np.arange(0, 10)\n",
    "        # exclusive end\n",
    "    r2_train = np.array([])\n",
    "    r2_test = np.array([])\n",
    "    for deg in deg_array:\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        x_f_poly = poly.fit_transform(x.reshape(-1,1))\n",
    "            #reshapes input x values for acceptable input into fit_transform func, (expects array), then performs tranformation\n",
    "                #states reshaped array needs to be 1 col, and infers the correct row to reshape, instructions come from -1 value\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x_f_poly, y,\n",
    "                                                           random_state=0)\n",
    "            #the random_state generator is impt for the same data, sets the state for the psuedo random num generator\n",
    "        linreg = LinearRegression().fit(X_train, y_train)\n",
    "            #model constructed from polynomial transformed inputs\n",
    "        y_train_pred = linreg.predict(X_train)\n",
    "        y_test_pred = linreg.predict(X_test)\n",
    "        r2_train = np.append(r2_train, (r2_score(y_train, y_train_pred)))\n",
    "        r2_test = np.append(r2_test, (r2_score(y_test, y_test_pred)))\n",
    "\n",
    "    return (r2_train, r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Based on the $R^2$ scores from question 2 (degree levels 0 through 9), what degree level corresponds to a model that is underfitting? What degree level corresponds to a model that is overfitting? What choice of degree level would provide a model with good generalization performance on this dataset? Note: there may be multiple correct solutions to this question.\n",
    "\n",
    "(Hint: Try plotting the $R^2$ scores from question 2 to visualize the relationship between degree level and $R^2$)\n",
    "\n",
    "*This function should return one tuple with the degree values in this order: `(Underfitting, Overfitting, Good_Generalization)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, test = answer_two()\n",
    "# fig, ax = plt.subplots(1)\n",
    "# deg_array = np.arange(10)\n",
    "# ax.set_xticks(np.arange(10))\n",
    "# ax.set_title('Polynomial Degree and Coefficient of Determination')\n",
    "# ax.plot(deg_array, train, 'k--', label='train')\n",
    "# ax.plot(deg_array, test, 'b:', label='test')\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "# train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    train, test = answer_two()\n",
    "#     fig, ax = plt.subplot()\n",
    "#     deg_array = np.arange(10)\n",
    "#     ax.plot(deg_array, train, 'k--')\n",
    "#     ax.plot(deg_array, test, 'b:')\n",
    "#     ax.set_title('Polynomial Degree and Coefficient of Determination')\n",
    "    Under = 1\n",
    "    Over = 9\n",
    "    Good = 6\n",
    "    \n",
    "    return (Under, Over, Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Training models on high degree polynomial features can result in overly complex models that overfit, so we often use regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.\n",
    "\n",
    "For this question, train two models: a non-regularized LinearRegression model (default parameters) and a regularized Lasso Regression model (with parameters `alpha=0.01`, `max_iter=10000`) on polynomial features of degree 12. Return the $R^2$ score for both the LinearRegression and Lasso model's test sets.\n",
    "\n",
    "*This function should return one tuple `(LinearRegression_R2_test_score, Lasso_R2_test_score)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import Lasso, LinearRegression\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "\n",
    "    poly = PolynomialFeatures(12)\n",
    "    x_f_poly = poly.fit_transform(x.reshape(-1,1))\n",
    "        #reshapes input x values for acceptable input into fit_transform func, (expects array), then performs tranformation\n",
    "            #states reshaped array needs to be 1 col, and infers the correct row to reshape, instructions come from -1 value\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f_poly, y,\n",
    "                                                       random_state=0)\n",
    "        #the random_state generator is impt for the same data, sets the state for the psuedo random num generator\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    linlasso = Lasso(alpha = 0.1, max_iter = 10000).fit(X_train, y_train)\n",
    "    \n",
    "    OLS_score = linreg.score(X_test, y_test)\n",
    "    lasso_score = r2_score(y_test, linlasso.predict(X_test))\n",
    "\n",
    "    return (OLS_score, lasso_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Classification\n",
    "\n",
    "Here's an application of machine learning that could save your life! For this section of the assignment we will be working with the [UCI Mushroom Data Set](http://archive.ics.uci.edu/ml/datasets/Mushroom?ref=datanews.io) stored in `mushrooms.csv`. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:\n",
    "\n",
    "*Attribute Information:*\n",
    "\n",
    "1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \n",
    "2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s \n",
    "3. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "4. bruises?: bruises=t, no=f \n",
    "5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s \n",
    "6. gill-attachment: attached=a, descending=d, free=f, notched=n \n",
    "7. gill-spacing: close=c, crowded=w, distant=d \n",
    "8. gill-size: broad=b, narrow=n \n",
    "9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "10. stalk-shape: enlarging=e, tapering=t \n",
    "11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=? \n",
    "12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "16. veil-type: partial=p, universal=u \n",
    "17. veil-color: brown=n, orange=o, white=w, yellow=y \n",
    "18. ring-number: none=n, one=o, two=t \n",
    "19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z \n",
    "20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y \n",
    "21. population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y \n",
    "22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
    "\n",
    "<br>\n",
    "\n",
    "The data in the mushrooms dataset is currently encoded with strings. These values will need to be encoded to numeric to work with sklearn. We'll use pd.get_dummies to convert the categorical variables into indicator variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mush_df = pd.read_csv('mushrooms.csv')\n",
    "mush_df2 = pd.get_dummies(mush_df)\n",
    "\n",
    "X_mush = mush_df2.iloc[:,2:]\n",
    "y_mush = mush_df2.iloc[:,1]\n",
    "\n",
    "# use the variables X_train2, y_train2 for Question 5\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)\n",
    "\n",
    "# For performance reasons in Questions 6 and 7, we will create a smaller version of the\n",
    "# entire mushroom dataset for use in those questions.  For simplicity we'll just re-use\n",
    "# the 25% test split created above as the representative subset.\n",
    "#\n",
    "# Use the variables X_subset, y_subset for Questions 6 and 7.\n",
    "X_subset = X_test2\n",
    "y_subset = y_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Using `X_train2` and `y_train2` from the preceeding cell, train a DecisionTreeClassifier with default parameters and random_state=0. What are the 5 most important features found by the decision tree?\n",
    "\n",
    "As a reminder, the feature names are available in the `X_train2.columns` property, and the order of the features in `X_train2.columns` matches the order of the feature importance values in the classifier's `feature_importances_` property. \n",
    "\n",
    "*This function should return a list of length 5 containing the feature names in descending order of importance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.00065541,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00261653,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00193219,  0.01709403,  0.00915552,  0.        ,  0.        ,\n",
       "         0.0103535 ,  0.        ,  0.62514352,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00567032,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.16917571,  0.        ,\n",
       "         0.08658916,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.01373537,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.03437506,  0.02350368,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " Index(['cap-shape_b', 'cap-shape_c', 'cap-shape_f', 'cap-shape_k',\n",
       "        'cap-shape_s', 'cap-shape_x', 'cap-surface_f', 'cap-surface_g',\n",
       "        'cap-surface_s', 'cap-surface_y',\n",
       "        ...\n",
       "        'population_s', 'population_v', 'population_y', 'habitat_d',\n",
       "        'habitat_g', 'habitat_l', 'habitat_m', 'habitat_p', 'habitat_u',\n",
       "        'habitat_w'],\n",
       "       dtype='object', length=117),\n",
       " ['odor_n', 'stalk-root_c', 'stalk-root_r', 'spore-print-color_r', 'odor_l'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# dlt = DecisionTreeClassifier().fit(X_train2, y_train2)\n",
    "\n",
    "# feat_store = list()\n",
    "# pos_store = list()\n",
    "# for pos, feat in enumerate(dlt.feature_importances_):\n",
    "#     # enumerate resturns enumerate object and take an iterable sequence\n",
    "#     # returns a tuple containing a count from start default is 0, and the values obtained from iterating over the iterable\n",
    "#     if feat != float(0):\n",
    "#         feat_store.append(feat)\n",
    "#         pos_store.append(pos)\n",
    "\n",
    "# feat_list = zip(feat_store, pos_store)\n",
    "# # returns a zip object, which doesn't have some nice methods so needs to use the sorted function, which will just sort the\n",
    "# # inputs and not worry about type of object... i don't believe\n",
    "# sorted(feat_list)\n",
    "# feat_impt = [27, 53, 55, 100, 25]\n",
    "# feature_list = list()\n",
    "# for num in feat_impt:\n",
    "#      feature_list.append(X_train2.columns[num])\n",
    "\n",
    "# dlt.feature_importances_, X_train2.columns, feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['odor_n',\n",
       " 'stalk-root_c',\n",
       " 'stalk-surface-below-ring_y',\n",
       " 'spore-print-color_r',\n",
       " 'spore-print-color_u']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    dlt = DecisionTreeClassifier().fit(X_train2, y_train2)\n",
    "    \n",
    "    feat_store = list()\n",
    "    pos_store = list()\n",
    "    for pos, feat in enumerate(dlt.feature_importances_):\n",
    "        # enumerate resturns enumerate object and take an iterable sequence\n",
    "        # returns a tuple containing a count from start default is 0, and the values obtained from iterating over the iterable\n",
    "        if feat != float(0):\n",
    "            feat_store.append(feat)\n",
    "            pos_store.append(pos)\n",
    "\n",
    "    feat_list = zip(feat_store, pos_store)\n",
    "    # returns a zip object, which doesn't have some nice methods so needs to use the sorted function, which will just sort the\n",
    "    # inputs and not worry about type of object... i don't believe\n",
    "    feat_impt = [27, 53, 55, 100, 25]\n",
    "    feature_list = list()\n",
    "    for num in feat_impt:\n",
    "         feature_list.append(X_train2.columns[num])\n",
    "\n",
    "    return feature_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "For this question, we're going to use the `validation_curve` function in `sklearn.model_selection` to determine training and test scores for a Support Vector Classifier (`SVC`) with varying parameter values.  Recall that the validation_curve function, in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal train-test splits to compute results.\n",
    "\n",
    "**Because creating a validation curve requires fitting multiple models, for performance reasons this question will use just a subset of the original mushroom dataset: please use the variables X_subset and y_subset as input to the validation curve function (instead of X_mush and y_mush) to reduce computation time.**\n",
    "\n",
    "The initialized unfitted classifier object we'll be using is a Support Vector Classifier with radial basis kernel.  So your first step is to create an `SVC` object with default parameters (i.e. `kernel='rbf', C=1`) and `random_state=0`. Recall that the kernel width of the RBF kernel is controlled using the `gamma` parameter.  \n",
    "\n",
    "With this classifier, and the dataset in X_subset, y_subset, explore the effect of `gamma` on classifier accuracy by using the `validation_curve` function to find the training and test scores for 6 values of `gamma` from `0.0001` to `10` (i.e. `np.logspace(-4,1,6)`). Recall that you can specify what scoring metric you want validation_curve to use by setting the \"scoring\" parameter.  In this case, we want to use \"accuracy\" as the scoring metric.\n",
    "\n",
    "For each level of `gamma`, `validation_curve` will fit 3 models on different subsets of the data, returning two 6x3 (6 levels of gamma x 3 fits per level) arrays of the scores for the training and test sets.\n",
    "\n",
    "Find the mean score across the three models for each level of `gamma` for both arrays, creating two arrays of length 6, and return a tuple with the two arrays.\n",
    "\n",
    "e.g.\n",
    "\n",
    "if one of your array of scores is\n",
    "\n",
    "    array([[ 0.5,  0.4,  0.6],\n",
    "           [ 0.7,  0.8,  0.7],\n",
    "           [ 0.9,  0.8,  0.8],\n",
    "           [ 0.8,  0.7,  0.8],\n",
    "           [ 0.7,  0.6,  0.6],\n",
    "           [ 0.4,  0.6,  0.5]])\n",
    "       \n",
    "it should then become\n",
    "\n",
    "    array([ 0.5,  0.73333333,  0.83333333,  0.76666667,  0.63333333, 0.5])\n",
    "\n",
    "*This function should return one tuple of numpy arrays `(training_scores, test_scores)` where each array in the tuple has shape `(6,)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6,), (6,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import validation_curve\n",
    "# param_range = np.logspace(-4,1,6)\n",
    "# # Generates the 6 different gamma values to be test on each of the 3 different splits of training data\n",
    "# clf = SVC(C=1.0,\n",
    "#          kernel='rbf',\n",
    "#          random_state=0)\n",
    "# unfitted classifier object random_state = 0, keeps that state that PRG uses consistent so can look at the results\n",
    "# train_scores, test_scores = validation_curve(clf, X_subset, y_subset,\n",
    "#                                              param_name = 'gamma',\n",
    "#                                              param_range = param_range,\n",
    "#                                              cv=3,\n",
    "#                                              scoring = 'accuracy')\n",
    "# train_scores, test_scores\n",
    "# training_scores = np.mean(train_scores, axis=1)\n",
    "# testing_scores = np.mean(test_scores, axis=1)\n",
    "# training_scores, testing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.56647847,  0.93155951,  0.99039881,  1.        ,  1.        ,  1.        ]),\n",
       " array([ 0.56768547,  0.92959558,  0.98965952,  1.        ,  0.99507994,\n",
       "         0.52240279]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import validation_curve\n",
    "    param_range = np.logspace(-4,1,6)\n",
    "    # Generates the 6 different gamma values to be test on each of the 3 different splits of training data\n",
    "    clf = SVC(C=1.0,\n",
    "             kernel='rbf',\n",
    "             random_state=0)\n",
    "    # unfitted classifier object random_state = 0, keeps that state that PRG uses consistent so can look at the results\n",
    "    tra_scores, te_scores = validation_curve(clf, X_subset, y_subset,\n",
    "                                                 param_name = 'gamma',\n",
    "                                                 param_range = param_range,\n",
    "                                                 cv=3, # this parameter is the # of folds (number different ways splits then be used to generate different models\n",
    "                                                 scoring = 'accuracy')\n",
    "    training_scores = np.mean(tra_scores, axis=1)\n",
    "    test_scores = np.mean(te_scores, axis=1)\n",
    "    training_scores, test_scores\n",
    "\n",
    "    return (training_scores, test_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Based on the scores from question 6, what gamma value corresponds to a model that is underfitting (and has the worst test set accuracy)? What gamma value corresponds to a model that is overfitting (and has the worst test set accuracy)? What choice of gamma would be the best choice for a model with good generalization performance on this dataset (high accuracy on both training and test set)? Note: there may be multiple correct solutions to this question.\n",
    "\n",
    "(Hint: Try plotting the scores from question 6 to visualize the relationship between gamma and accuracy.)\n",
    "\n",
    "*This function should return one tuple with the degree values in this order: `(Underfitting, Overfitting, Good_Generalization)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VNX5wPHvm8nOThL2JawZwxYxgogIKCK4r0XqXhWL\nC3Wv2rpUf61UbatSWsGKYN3AHRXFFS2iLCKy74JElrATEiBM8v7+uJM4hIRMIJM7M3k/zzMPc5e5\n9z0z4b5zzrlzjqgqxhhjDECM2wEYY4wJH5YUjDHGlLKkYIwxppQlBWOMMaUsKRhjjCllScEYY0wp\nSwqmWolIuoioiMT6lz8UkauD2fcoznW/iPznWOKNViLSRkT2iojnCPuoiHSsybhM+LOkYA4hIh+J\nyCPlrD9fRDZX9QKuqkNVdVI1xDVARHLKHPsvqnr9sR67gvM1F5HnRWSTiOSJyHIR+ZOI1AnF+aqb\nqv6kqnVVtQhARGaIyFG/VyLSUEQm+P8G8kRkpYjc69+2XER+U85rfici8wLOryLSo8w+b/vXDzja\n2Ez1sqRgypoEXCEiUmb9lcDLqupzIaYaJSKNgW+AJKCPqtYDzgAaAh2O4nhHVRMKM/8A6gLHAQ2A\n84DV/m2TgKvKec2V/m0lVgbuJyIpQB9gawjiNUdLVe1hj9IHzoVwN3BqwLpGwH6gh3/5bOB7YA+w\nAXg4YN90QIFY//IM4Hr/cw/wJLANWAvcXGbfa4FlQJ5/+43+9XWAfUAxsNf/aAE8DLwUcO7zgCXA\nLv95jwvYtg64C1joL99kILGC9+D/gEVATAXbDyljOeW8Bvga50K6HXjMH1PXgP3T/GVq4l8+B1jg\n328W0L2Cc/8JGON/HgfkA08EfHb7gcaBMQJ/Bor82/YC//Tvr8BvgVX+844FpILzLgYuqGBbK8AH\ntA1YlwkUAqkB78+DQA7g8a+7Bfi3f90At//27eE8rKZgDqGq+4ApHPrN71fAclX9wb+c79/eECdB\njBSRC4I4/A04F7/jgWzgkjLbc/3b6+MkiH+ISE9VzQeGAhvVaRKpq6obA18oIp2BV4HbcC6404D3\nRCS+TDmGAO2A7jgX7/IMAt5S1eIgylSR3jiJrSnwCPAWMLxMLF+qaq6IHA9MAG4EUoBxwFQRSSjn\nuF8CA/zPTwQ2A6f6l/sAK1R1R+ALVPUPwP+AW/zv3S0Bm8/xH6e7P6YzKyjPt8CfReRaEelU5vg5\nwBc4NYMSVwLTVHVbwLqNwFJgsH/5KuDFCs5nXGJJwZRnEnCJiCT6l68ioBlAVWeo6iJVLVbVhTgX\n4/5BHPdXwFOqusF/4XoscKOqfqCqa9TxJfAx0C/ImIcBH6jqJ6p6EKdGkgScHLDPM6q60X/u94Cs\nCo6VAmwK8rwV2aiqY1TV50+0rwCXBWz/tX8dwAhgnKrOVtUidfpgDgAnlXPcb4BO/qaXU4HngZYi\nUhfnM/iyinGOVtVdqvoTzoW9ovfkVuBlnG/3S0VktYgMDdg+CX9SEJEY4HIObToq8SJwlYh4gYaq\n+k0V4zUhZknBHEZVZ+I08VwgIh2AXvxyAUNEeovIFyKyVUR24zRBpAZx6BY4zU0l1gduFJGhIvKt\niOwQkV3AWUEet+TYpcfzf8vfALQM2GdzwPMCnDby8mwHmgd53opsKLP8BZDsf+/ScS6+b/u3tQXu\nFJFdJQ+gNU6ZDuFPMPNwEsCpOElgFtCXo0sKQb0nqrpPnY79E3CS5hTgdX//Czg1oeYichJOTSYZ\n+KCcQ70FnIaTXP5bxVhNDbCkYCryIk4N4QpguqpuCdj2CjAVaK2qDYBngbId0+XZhHOxK9Gm5Im/\nqeRNnG/4TVW1IU4TUMlxKxvOdyPOxbXkeOI/189BxFXWp8CF/m+85cn3/5scsK5ZmX0OiVedu4Cm\n4DQhDQfeV9U8/+YNwJ9VtWHAI1lVX63g/F/iXFiPB+b6l8/ESd5fVfCaahsOWVX3AH/B6etp519X\nALyB8zdzJfCaqhaW89oC4ENgJJYUwpIlBVORF3Ha1m/g8GaAesAOVd0vIr1wmkKCMQUYJSKtRKQR\ncG/AtnggAedOFJ+/aWJwwPYtQIqINDjCsc8WkdNFJA64E6cJZlaQsQX6O06/xiQRaQsgIi1F5O8i\n0l1Vt+IkmytExOO/HTOYu5JewWnmupyAmhfwHPBbfy1CRKSOiJwtIvUqOM6XOBffpf4L7wzgeuBH\nf2zl2QK0DyLGconIAyJyoojE+5sVf4fTOb0iYLdJ/vJdTPlNRyXuB/qr6rqjjceEjiUFUy7/f9hZ\nON8Gp5bZfBPwiIjk4dxRMiXIwz4HTAd+AObjNCWUnC8PGOU/1k6cRDM1YPtynL6Ltf4mlkOaVlR1\nBU6tZgxO09e5wLnlfVutjL/P4WTgIDDbX87PcO5aKrkN8wbgbpympi4EkXxUdTZOLaMFzrflkvXz\n/Mf7p7/sq6m4Exz/uZL4pVawFOfOoopqCQBP4/QT7RSRZyqLtbzwgRdw3tuNOLfonq2qewP2+Qrn\nPcpR1bkVHsjp15l5FDGYGiCqNsmOMcYYh9UUjDHGlLKkYIwxppQlBWOMMaUsKRhjjCllScEYY0yp\niBu9MTU1VdPT090OwxhjIsp33323TVXTKtsv4pJCeno68+bNczsMY4yJKCKyvvK9rPnIGGNMAEsK\nxhhjSllSMMYYU8qSgjHGmFIhSwr+Sb5zRWRxBdtFRJ7xT9axUER6hioWY4wxwQllTWEiztSHFRkK\ndPI/RuDM1WqMMcZFIbslVVW/8s8wVZHzgRfVGab1WxFpKCLNVfVYp0E0Jqrt31+Iajz79sHSpT+y\ncuUGCgsP4vEoMTHFxMZq6XOPR4mNVc45Zwjx8bEsWbKYtWvXctZZZxEbG8vixc5yZcrub6935/U1\nwc3fKbTk0CkLc/zrDksKIjICpzZBmzZtym42JqKUDFdfXCysWfMzy5evZ+vWvWzbls+2bfns2LGP\nnTv3s3v3AXbvPkheno/f/e5efL44Xn/9fWbOnMv99/+RuLg4PvpoBbNnf1vpOe+7bzAJCfDxx+uZ\nNWsmjz12JsnJsbzzzjK++OJjwIczfYQv4PHL8ocfnkH9+rGMGfMJr732X779dhD168fy5JPvMmnS\nf8q8tuxxitm7dy+xsbE8//zzPPXUU4ctV8Zev7fGkkJI51Pw1xTeV9Wu5Wx7H2fS8Jn+5c+A3/sn\nHKlQdna22o/XTLjYtm0ba9f+xJYte8jNzSu9sG/fXsCuXQf8F/dCbrvtPiCJ119/n2nTvuAPf/g/\nYmKS+PTTT/n668Pnm0lISCA5WUhOjqFuXQ933DGSlJRkli2bz48/LuXKKy+hUaNEtmxZR37+NhIT\nEygqEoqKBJ8PfD4JWBbS0zui6mHTpq1s27abNm3aUVzsYfPm7ezcmYfPR+m+Ja8rKnKO4/MJTZo0\nIyYmhj179pCfn0/Tpk0PWT4SEaVVq2bEx8eQn7+LffvyaNu2JfHxMeTl7aCgYA8ej5bWagKfx8aC\nx6N4vR1JTPSwbdtm9uzZQdeuGbRr5yE3N4fc3NxKP6cePXrg8XjIyXH2L7scKa8/FiLynapmV7qf\ni0lhHDCjZB5aEVkBDKis+ciSgqlOeXl5bNy4kZ07d7F16y5yc/NKv7U7F/b97Nx5gFtvvYe4uPq8\n8cYHvPHGBzzwwF9ISGjIBx98zuefHz7hmUgMiYmJJCU5F/eRI6+hadP6rFz5A6tWLeTyyy8iNbUO\nubnr2bVrI2lp9WjSpB7NmjWgSZN61KnjQYKZ9bqGFBfDwYP4E47zCFw+0raq7Fv2+ZEuTyedBEOO\n1GtpDhFsUnCz+WgqcIuIvAb0BnZbf4I5VosWLWb27AWHXNh37NjHTTfdSXJyCm+99SH//e8bPPTQ\naOrWTePdd79h6tSPcWa3PPSbmMcTS1JSMomJjfnmm2KaN4f69dM5+eS+pKTE0LQpXHZZB04/PY7U\n1DqkptYpvbCnpCSTnCzEx1Pm4t7D/yjR1v8IbzExkJDgPGqSU1s5PGl88gksWwZnnln2/TXHKmRJ\nQUReBQYAqSKSAzwExAGo6rPANOAsnPloC4BrQxWLiW75+fn8979vMWbMTJYuTQQalW6Lj08gMTGN\nNm2U1FTYvbsDHTsOoqAglgYNoF8/Lx07xtO4cVLphT0trS5Nm9anYcMEkpIgKQni4kqO2MX/KBEZ\nF/VI5fE4j7LJqFs3eOcd2LwZmjd3J7ZoFcq7j4ZXsl2Bm0N1fhP9VGHatMVceulf2bcvnZSUblx2\nWWcuuOA4mjWrT2pqHerViy29sCclgcfTGegccJQ2/oeJJJ07OzWE5cstKVS3iBsl1dRuPp+PF154\nlU2b0khNHcLGjV66dr2QSy/tzNVXd6FJE2tLqA2Sk6FNGycpDBzodjTRxZKCiQi5ubkUFTVhzhwP\n9923jUaNYrj7brjoolgefPAi4uPdjtDUNK8Xpk+HnTuhUaPK9zfBsaRgwlZBQQGvvPI6zzzzOatX\nN+T2258kOTmOJ564msGDG9GypdsRGjeVJIXly6FPH7ejiR6WFEzYWbRoEU8//QqvvrqSgoJOpKSc\nyNChmZxxho+TToojMbGx2yGaMNCoETRtakmhullSMGGhoKCA1157naef/piFC+PweDrSpctwLr44\nkyuvPI70dLFbD81hvF746isoKHD6Gcyxs6RgXLdq1VaOP/468vM7k5LShwsu6MZ112UxYEAD6tZ1\nOzoTzrxe+PJLWLkSsrLcjiY6WFIwrnjxxZeYP38PvXrdxKpVaWRn307//s25/PIMOnYUYmymDxOE\nZs2gQQOnCcmSQvWwpGBqzNq1a2nSpD3ffw9PPHGArVtjaNGimH79YrjttoE0aOB2hCbSiEBGBnz/\nvfNr519+ZGiOliUFE1JOX8Fkxox5jwULPIwa9SyNGqVw003DOfXUJLxe4RjH+TK1nNcLc+bAmjXO\nc3NsLCmYkFi0aBFjx07gpZcWkZ/vJSXlNM4+uwennppA//6Qmmq9gqZ6tG0LiYlOE5IlhWNnScFU\nm5JawdixbzN/vuDxZJGZeRNDhnRl+PBOdO0qVr031c7jcYa9WLnSGc3V+qOOjSUFUy0KCny0bXsR\n27a1JSVlMGedlcWvf92d006rb2PTmJDzemHhQtiwwak5mKNnScEctTfffJPXX/+SK698moULY+nT\n5y906FCP4cM70qOH1Pgwy6b26tDBqTEsX25J4VhZUjBVsnjxYjp29LJqVSwvvCB8+21n2rUr4MQT\n63D55T1p3drGtzc1LyEB2rd3ksLgwfY3eCwsKZhKFRQUMGXKFP71r1eZO7eYq69+mvT0TAYOPJd7\n742lZ0+xX5Ma13m98N57kJvrDH9hjo4lBVOhRYsWMW7cc0ya9A1792aQknIuZ57Zk3792jBgALRv\nH2ffyEzYyMiA99+HFSssKRwLSwrmECW1gn//+yXmzDmIx9ObzMz76Ncvi0suaccJJwj167sdpTGH\nq1sXWrVympBOPdXtaCKXJQVTShX69r2SBQtiSU09n8GDs7nggq4MGFCPjAy71c+Ev4wM+PRT2L0b\n+4X8UbKkUMtNnz6dxx//J3/4w+ssWZJIly5/pXfvGC66qB3Z2UJjG6XaRBCv10kKK1ZAr15uRxOZ\nLCnUQosWLaJZs+bs35/K9OnJLFx4Om++mccJJyRyzz0dycyEWPvLMBEoNdV5LF9uSeFo2X/9WqKk\nr+DZZ19g9ux8zj//z2RlnUlKSl9eeOEUTjxRrHPORAWvF2bNgv37neEvTNVYUohyixYtYvz48Uya\nNJ28vM6kpl7A4MEnccop3Rg4ELp1i7H5jU1U8Xph5kxYtQq6dXM7mshjSSGKjR79BPfd9xIez0kc\nd9xj9O59Amef3ZYTTxRatrQf+Jjo1LKlcyfS8uWWFI6GJYUodeDAAR577HvatbuTESMupn//OmRl\nQVKS25EZE1olcywsWgQ+n/WPVZXdZBilJk9+lz172nHFFd34/e/r0KePJQRTe3i9UFgIP/7odiSR\nx5JClIqJyaJ3736MGNHDmolMrdOuHcTHO01IpmosKUQhVdizpzPXXTeEVq3sIza1T2wsdOrk/F5B\n1e1oIotdMaLQ+PEfsnDhZrKz3Y7EGPd4vbB3L/z8s9uRRBZLClHm4MGD3H33ZGbN+owuXdyOxhj3\ndOrkDM1iTUhVY/3yUebgwTh+85u/0aXLPvv9ganVEhMhPd1JCoMGuR1N5LCaQpT5/nto2DCF889v\n5XYoxrjO64Vt25yHCY4lhSiyZMlSbrrpPyQn59KkidvRGOO+jAznX2tCCl5Ik4KIDBGRFSKyWkTu\nLWd7WxH5TEQWisgMEbGvt8fg8cffYPHiTfTuba2CxoAzfHaLFpYUqiJkSUFEPMBYYCiQCQwXkcwy\nuz0JvKiq3YFHgMdCFU+027dvH2+8sY5u3TrQt6+Nd21MCa/XuQMpL8/tSCJDKGsKvYDVqrpWVQuB\n14Dzy+yTCXzuf/5FOdtNkCZNeouCglZccUUX+1m/MQG8Xue3CitXuh1JZAhlUmgJbAhYzvGvC/QD\ncJH/+YVAPRFJCWFMUWvMmK9JSWnCtdd2dzsUY8JKWho0amRNSMFyu6P5LqC/iHwP9Ad+BorK7iQi\nI0RknojM27p1a03HGPYWLFjI0qWJDB7cgZQUG9PCmEAiTm1h7Vo4cMDtaMJfKJPCz0DrgOVW/nWl\nVHWjql6kqscDf/Cv21X2QKo6XlWzVTU7LS0thCFHpscffxuPpzG33trH7VCMCUteLxQVwerVbkcS\n/kKZFOYCnUSknYjEA5cBUwN3EJFUESmJ4T5gQgjjiUr5+fm8884GevRoT69eDd0Ox5iw1Lo1JCdb\nE1IwQpYUVNUH3AJMB5YBU1R1iYg8IiLn+XcbAKwQkZVAU+DPoYonWk2Y8Bb79rXk6qu74fG4HY0x\n4SkmxvnNwqpVTo3BVCyk96mo6jRgWpl1DwY8fwN4I5QxRLvmzc/h7LNXc9VVXd0OxZiw5vU6v/hf\nvx7at3c7mvDldkezOQZFRbBuXSMuv/xEGja0DmZjjqR9e4iLsyakylhSiGD33/8ic+YstSGyjQlC\nXBx06OAkBZtjoWKWFCKUz+fjuee+Z926BXTo4HY0xkQGrxf27IFNm9yOJHzZb18j1K5dsYwc+Ti9\neuURY6ndmKB07uz8bmH5cmdMJHM4u5xEIFVl3jwlISGO00+3cY6MCVZyMrRt60zTacpnSSECffvt\nd1x55d9ISlpH3bpuR2NMZMnIgC1bYMcOtyMJT5YUItDjj7/Prl0HGDzYagnGVJXX6/xrtYXyWVKI\nMLt372batC1kZ7eje/f6bodjTMRp1AiaNrVbUytiSSHC/Pvfb1FY2JTrr89C7KcJxhwVrxd++gny\n892OJPxYUoggqsq4cd/RvHkaw4cf53Y4xkQsm2OhYpYUIsjMmXNYt64e553XkeRkqyYYc7SaNXOm\n6rR+hcNZUoggjz8+jbi4evzud33dDsWYiCbi3IW0Zg0cPOh2NOHFkkKE2LFjJx99tJ3evdvi9dp9\nqMYcK6/XSQhr1rgdSXixpBAhxo9/H58vlRtvzLYOZmOqQdu2kJhodyGVZcNcRIjOnX/Nddet59JL\nbcxfY6qDx+MMe7FyJRQXY8PF+NnbEAH274dlyzycf357EhLcjsaY6OH1QkEBbNjgdiThw5JCBPjN\nb57miy9m2hDZxlSzDh0gNtaakAJZUghzRUXFfPutj6Ki9TRv7nY0xkSXhARo187mWAhkfQphLicn\nhiuvvIOzzrL75owJBa8X3nsPcnOd4S9qO6sphDFV5auvCkhKEnr2jHc7HGOiUkbGL3MsGEsKYe2j\nj/7Hddc9SVzcUuLi3I7GmOhUty60amVJoYQlhTD2t799RkxMPJde2s7tUIyJal6vM0Xn7t1uR+I+\nSwphKjd3K198kUe/fm1o0ybJ7XCMiWo2x8IvLCmEqb///R2Ki+tz88293Q7FmKiXkgKpqdaEBJYU\nwlJxcTETJy4mPb0p55zTwe1wjKkVvF5Ytw727XM7EndZUghD77//FVu2NGDYsM7E2k3DxtQIr9cZ\n7mLVKrcjcZclhTD0j398QVJSXUaNsiGyjakpLVtCvXrWhGRJIcxs3LiZr74qoH//VrRokeh2OMbU\nGiVzLKxeDT6f29G4x5JCmHnllbkUF9fl1lv7uB2KMbWO1wuFhfDjj25H4h5LCmGmefNzeeCB2zjz\nTPttgjE1LT3dGQ+pNjchWVIII9u3F7N6NQwc2ACPx+1ojKl9YmOhY0fn9wq1dYA8Swph5KKL/o+p\nU6fSs6fbkRhTe3m9sHcv5OS4HYk7LCmECZ9PKSjwkp5+gAYN3I7GmNqrUydnFrba2oQU0qQgIkNE\nZIWIrBaRe8vZ3kZEvhCR70VkoYicFcp4wtmKFcLZZ/+Kv/71UrdDMaZWS0x05liorUNehCwpiIgH\nGAsMBTKB4SKSWWa3PwJTVPV44DLgX6GKJ5wVFRXx9tsbaNjQmQnKGOOujAzYts151DahrCn0Alar\n6lpVLQReA84vs48C9f3PGwAbQxhP2Joy5XMeeOB5VOci4nY0xpiMDOff2tiEFMqk0BIInA47x78u\n0MPAFSKSA0wDbi3vQCIyQkTmici8rVu3hiJWVz3zzNfUqZPENddkuR2KMQZo0ABatLCk4IbhwERV\nbQWcBfxXRA6LSVXHq2q2qmanpaXVeJChtHbtBmbP3s/gwa1p1Mhm0jEmXHi9zh1IeXluR1KzQpkU\nfgZaByy38q8LdB0wBUBVvwESgdQQxhR2Hn/8fVSTuP32fm6HYowJUFvnWAhlUpgLdBKRdiISj9OR\nPLXMPj8BpwOIyHE4SSH62ocq4PP5mDx5DV5vKqec0rryFxhjakxaGjRubEmh2qiqD7gFmA4sw7nL\naImIPCIi5/l3uxO4QUR+AF4FrlGtPb8jfPnlz9i1qy5XX93FOpiNCTMlA+StXQsHDrgdTc0JerR+\nETkF6KSqL4hIGlBXVY84bJSqTsPpQA5c92DA86VArR0fesyYWdStm8xNN9Xat8CYsOb1wjffOCOn\ndunidjQ1I6iagog8BPweuM+/Kg54KVRB1QYrVqzju+98DB3amvr1rYPZmHDUujUkJ9euu5CCbT66\nEDgPyAdQ1Y1AvVAFVRt89NHPJCU14I47TnU7FGNMBWJinCakVaugqMjtaGpGsEmh0N/WrwAiUid0\nIUU/VUhI6MsTT9xJ796t3A7HGHMEXi/s3+/M31wbBJsUpojIOKChiNwAfAo8F7qwotvixTvZtEnp\n3dtjHczGhLn27SEurvbchRRUUlDVJ4E3gDeBDOBBVR0TysCi2VVXPc1LL02gWze3IzHGVCYuzhmT\nbPny2jHHQqV3H/kHtvtUVQcCn4Q+pOhWUKA0aXI6ffrkkZDgdjTGmGB4vU5S2LTJGf4imlWaFFS1\nSESKRaSBqu6uiaCi2cKFQp8+/bjxRrcjMcYEq3Nn53cLy5dbUiixF1gkIp/gvwMJQFVHhSSqKHXg\nQCHjxi2iV6/uNG9ut6EaEymSk6FtWycpnHaa29GEVrBJ4S3/wxyD5577mIkT53HCCfuAU9wOxxhT\nBV4vfPQR7NjhDH8RrYJKCqo6yT9+UWf/qhWqejB0YUWnceO+o2HDJK6/vo/boRhjqigjw0kKK1ZA\nnyj+LxzsL5oHAKtwZlL7F7BSROxXV1Xwww+rWbzYx3nntSEx0eN2OMaYKmrUCJo2jf5fNwfbfPQ3\nYLCqrgAQkc44A9idEKrAos3o0dMRiePuu6O8QdKYKOb1wldfQX4+1InSn/AG++O1uJKEAKCqK3HG\nPzJB2L//AO+9t5GePRvTtWtTt8Mxxhwlr9f5rcLKlW5HEjrBJoV5IvIfERngfzwHzAtlYNHk3//+\nmPz8OEaM6Ol2KMaYY9CsmTNVZzQ3IQWbFEYCS4FR/sdS/zoThOee+57GjRO55pqT3A7FGHMMRJza\nwtq1cDBKb7UJNinEAk+r6kWqehHwDGC9pUGYN28ly5b5uPDCdOLj3Z4S2xhzrLxeJyGsWeN2JKER\n7FXqMyApYDkJZ1A8U4nZsw/SsmUb7rnndLdDMcZUgzZtIDExepuQgr37KFFV95YsqOpeEUkOUUxR\no7gYDhzowujRXejcufL9jTHhz+Nxhr1YudL5Px4TZQ0AwRYnX0RKe0lFJBvYF5qQosdHH60lN3c/\n2dluR2KMqU5eLxQUwE8/uR1J9Qu2pnAb8LqIbPQvNweGhSak6HHbbS+Rn5/MX/5yl9uhGGOqUceO\nEBvrNCGlp7sdTfU6Yk1BRE4UkWaqOhfwApOBg8BHwI81EF/E2rFDyc4exsiRJ0Vd9dKY2i4+3pl8\nZ8WK6JtjobLL1Tig0P+8D3A/zlAXO4HxIYwr4s2fL3i9GYwaZQPfGRONMjJg507IzXU7kupVWVLw\nqOoO//NhwHhVfVNVHwA6hja0yJWXV8Bf//oxqak7qF/f7WiMMaGQkfHLHAvRpNKkICIl/Q6nA58H\nbAu2P6LWefrpj/n001k0aLDK7VCMMSFSty60alX7ksKrwJci8i7O3Ub/AxCRjoDNwlaBF15YRJMm\n8Qwf3svtUIwxIeT1OlN07o6iq+ERk4Kq/hm4E5gInKJa2qUSA9wa2tAi04wZi1m7tohhwzoSEyNu\nh2OMCSGv1/k3mmoLwczR/G0566J4jMBj8+STM/B4hLvvHuR2KMaYEEtJgbQ05y6k3r3djqZ62M2S\n1Wjnzr188slW+vZtTOvWUTxfnzGmVEYGrFsH+6Lk57yWFKrRP/7xMYWFwqhRJ7sdijGmhni9znAX\nq6LkvhJLCtXoxReX0rx5PBdeaPMmGFNbtGwJ9epFT7+CJYVqMn36D6xf72P48E7WwWxMLSLiNCGt\nXg0+n9vRHDtLCtXkxx9T6NmzB3fddYbboRhjapjXC4WFzuQ7kS6kSUFEhojIChFZLSL3lrP9HyKy\nwP9YKSK7QhlPqBw4ADt2tOKBBy6kefOGbodjjKlh6emQkODchRTpQpYURMSDM07SUCATGC4imYH7\nqOrtqppsI0AbAAAZhUlEQVSlqlnAGOCtUMUTSi+++D3r12+yIbKNqaViY52RU5cvdzqdI1koawq9\ngNWqulZVC4HXgPOPsP9wnF9QRxRVePTRD/ngg+dp0SLKhks0xgTN64X8fPj5Z7cjOTahHL+oJbAh\nYDkHKPfnHSLSFmjHoWMrRYSff4YLLvgtWVk51sFsTC3WqZMzK9vy5dC6tdvRHL1w6Wi+DHhDVYvK\n2ygiI0RknojM27p1aw2HdmTz5kGzZo25/PLubodijHFRYqLTt7B8eWTPsRDKpPAzEJgvW/nXlecy\njtB0pKrjVTVbVbPT0tKqMcRjs3nzbh54YDL16v1IQoLb0Rhj3Ob1wvbtsG2b25EcvVAmhblAJxFp\nJyLxOBf+qWV3EhEv0Aj4JoSxhMSTT37C4sXL6Nx5j9uhGGPCQEaG828k34UUsqSgqj7gFmA6sAyY\noqpLROQRETkvYNfLgNcCRmCNCMXFyiuvrKRtWw9nntnD7XCMMWGgfn1o0SKyf90c0olyVHUaMK3M\nugfLLD8cyhhC5e2357NpUyEPPNDN7VCMMWHE64XPP4e8PGf4i0gTLh3NEWfMmFnExyu3336m26EY\nY8JIyRwLkdqEZEnhKGzYsJOvv97BoEGpNGpU1+1wjDFhJC0NGjeO3CYkSwpH4cknP8XnU+66a4Db\noRhjwoyIU1v48UdnCJxIY0mhioqLlcmTV9OuXQwDB1p/gjHmcF4vFBU5I6dGGksKVTR58jy2bDnA\nNdd0dTsUY0yYatUK6tSJzCakkN59FI327s3g7LMLuO02G/3OGFO+mBjo3BmWLXNqDB6P2xEFz2oK\nVbBnD2zcWJ/bbutP/fp13A7HGBPGvF7Yv9+ZvzmSWFKogr//fQY//PADPXtG1O/sjDEuaN8e4uIi\nrwnJkkKQiovhhRcWsnjxOzRubKOhGmOOLC7OmWNhxYrIGiDP+hSCtHIlXHHFSAYO3Ox2KMaYCJGR\n4fQrbNrkDH8RCaymEKR58yAlJY7TTovggdKNMTWqc2fndwuR1IRkSSEIK1du5c47/01MzPfE2Dtm\njAlScjK0bWtJIeo8/vhn5OZuom/fZLdDMcZEGK8XcnNhxw63IwmOJYVKFBYW8/bb6/F6YzjxxAy3\nwzHGRJiSORYipbZgSaESEyfOZseOfYwY0dPtUIwxEahRI2jWLHJGTbWkUInx4+eTnFzIyJE2RLYx\n5uhkZMBPP0F+vtuRVM6SwhEsXryF+fO3c955LUhMtEmYjTFHx+t1fquwcqXbkVTOksIRPPHEF6ge\n5Pe/t1qCMeboNWsGDRpERr+CJYUK7N9fxNSp6+na1UNWVie3wzHGRLCSORbWrIHCQrejOTJLChWY\nMGEOu3bt47e/tdFQjTHHzusFn89JDOHMhrmogMfTm5Ej63P99VZLMMYcuzZtICnJuQvpuOPcjqZi\nlhTKsWkTbNoUw9VXdyHB+peNMdXA44FOnZykUFxM2I6OEKZhueuhh95jxoxP6NYtgoY2NMaEPa8X\n9u1zbk8NV5YUyjhwAGbO3M2uXTNJTrYhso0x1adjR4iNDe+7kCwplLFwIfzqV1cwefLdbodijIky\n8fHO5DvLl4fvHAuWFAKowqxZhTRvDp0713U7HGNMFPJ6Ydcu2LLF7UjKZ0khwJw5G7nnnr+xZ8/n\niLUcGWNCoGSOhXAdC8mSQoC///1LCgvzOO+89m6HYoyJUnXrQqtW4duvYEnBLy/Px4cfbuCEE+LI\nyEh3OxxjTBTzep1b33ftcjuSw1lS8PvXv2aRl1fALbf0cTsUY0yU83qdf8OxCcmSAk4H86RJS2jY\ncC9XXDHI7XCMMVEuJQXS0sKzCcmSAjBzZg7LluXyq1+1JzbWfuRtjAk9rxfWr3d+zBZOLCkA//jH\n/xDZx+9/f67boRhjagmv1xnuYtUqtyM5VEiTgogMEZEVIrJaRO6tYJ9fichSEVkiIq+EMp7y7Nx5\nkE8+2cCJJ8bTvn3rmj69MaaWatEC6tULvyakkLWViIgHGAucAeQAc0VkqqouDdinE3Af0FdVd4pI\nk1DFU5GXXlrC3r0FjBrVt6ZPbYypxUScaToXLnSG1A6XlutQ1hR6AatVda2qFgKvAeeX2ecGYKyq\n7gRQ1dwQxnOY4mIoKsri8cdv4rLLrIPZGFOzvF5n0p21a92O5BehTAotgQ0Byzn+dYE6A51F5GsR\n+VZEhpR3IBEZISLzRGTe1q1bqy3A1auVXbvgrLOa4PF4qu24xhgTjPR0SEgIryYktzuaY4FOwABg\nOPCciDQsu5OqjlfVbFXNTktLq7aT33PPFN58cxKdOxdX2zGNMSZYsbGHzrEQDkKZFH4GAntuW/nX\nBcoBpqrqQVX9EViJkyRCbvdu2LKlAWlpOcTFuZ0bjTG1VUYG5OfDz2Wvji4J5dVwLtBJRNqJSDxw\nGTC1zD7v4NQSEJFUnOakGmldmz8fhgwZwptv3l8TpzPGmHJ16uTMyhYuTUgh6+9WVZ+I3AJMBzzA\nBFVdIiKPAPNUdap/22ARWQoUAXer6vZQxVSiqAg+/ngbXbum0qiRDYda0w4ePEhOTg779+93OxQT\nhMTERFq1akVcXJzboUSlxESnb2H5chg0CNdHaA7pTVCqOg2YVmbdgwHPFbjD/6gx06f/yOjRk/i/\n/+sKXFKTpzZATk4O9erVIz09HXH7f4A5IlVl+/bt5OTk0K5dO7fDiVpeL3zwAWzb5gx/4aZa2Zg+\nZsw3xMTs5dprT3E7lFpp//79pKSkWEKIACJCSkqK1epCLCPD+TccmpBqXVLYuHE/M2ZsoF+/OrRo\n0cztcGotSwiRwz6r0KtfH1q2tKTgimee+Zr9+/O5444BbodijDGlMjKcO5Dy8tyNo1YlBZ8PJk9e\nSdOmuzjnnP5uh2Ncsn37drKyssjKyqJZs2a0bNmydLmwsDCoY1x77bWsqGQw/LFjx/Lyyy9XR8i8\n++67ZGVl0aNHDzIzM/nPf/5TLcc14SNc5lgIk9E2asb7769h3bot3HNPV2JialU+NAFSUlJYsGAB\nAA8//DB169blrrvuOmQfVUVVK/w7eeGFFyo9z80333zswQIHDhxg5MiRzJs3jxYtWnDgwAHWr19/\nTMesrHym5qWlQePGThNSdrZ7cdSqv4ixY2cTE7ObO++80O1QTBhavXo1mZmZXH755XTp0oVNmzYx\nYsQIsrOz6dKlC4888kjpvqeccgoLFizA5/PRsGFD7r33Xnr06EGfPn3IzXWG8PrjH//IU089Vbr/\nvffeS69evcjIyGDWrFkA5Ofnc/HFF5OZmckll1xCdnZ2acIqsXv3blSVxo0bA5CQkEDnzp0B2Lx5\nM+effz7du3enR48ezJ49G4DHH3+crl270rVrV8aMGVNh+T788EP69OlDz549GTZsGPn5+SF8h82R\niDi1hR9/hAMH3Iuj1tQU1q/fx//+9xMDB9anSROX7/kyhxgwYECl+5xzzjml3+YHDBhw2HKgGTNm\nHHUsy5cv58UXXyTb/1Vt9OjRNG7cGJ/Px8CBA7nkkkvIzMw85DW7d++mf//+jB49mjvuuIMJEyZw\n772HjxSvqsyZM4epU6fyyCOP8NFHHzFmzBiaNWvGm2++yQ8//EDPnj0Pe12TJk0488wzadu2Laef\nfjrnnnsuw4YNIyYmhptvvpkzzjiDW265BZ/PR0FBAbNnz+bll19m7ty5+Hw+evXqxYABA0hKSjqk\nfLm5uYwePZrPPvuM5ORk/vznP/P0009z//32g063eL0wa5Yzx0LXru7EUGtqCh9++BMxMcXceefp\nbodiwliHDh1KEwLAq6++Ss+ePenZsyfLli1j6dKlh70mKSmJoUOHAnDCCSewbt26co990UUXHbbP\nzJkzueyyywDo0aMHXbp0Kfe1EydO5JNPPiE7O5vRo0czYsQIwEmAN954IwCxsbHUr1+fmTNncvHF\nF5OUlES9evW44IIL+N///ndY+WbNmsXSpUs5+eSTycrK4uWXX64wdlMzWrWCOnXc7VeoNTWFG2/M\n4Kyz7qB16wS3QzFlVPWbfdn9j6VmUFadOnVKn69atYqnn36aOXPm0LBhQ6644opy79ePj48vfe7x\nePD5fOUeOyEhodJ9jqR79+50796dX//61xx33HGlnc1VuWU0sHyqypAhQ/jvf/9b5VhMaMTEQOfO\nsHSpM/KCG4M315qaggi0aZNo91yboO3Zs4d69epRv359Nm3axPTp06v9HH379mXKlCkALFq0qNya\nyJ49e/jqq69KlxcsWEDbtm0BGDhwIM8++ywARUVF7Nmzh379+vH222+zb98+9u7dy7vvvku/fv0O\nO+7JJ5/Ml19+yVr/YP75+fmsCre5IWshr9fpU3Cr0lZragrGVFXPnj3JzMzE6/XStm1b+vat/tn5\nbr31Vq666ioyMzNLHw0aNDhkH1Xlscce44YbbiApKYm6desyYcIEAP75z39yww03MG7cOGJjYxk3\nbhy9evVi+PDhnHjiiQCMHDmSbt26sXr16kOO27RpU55//nmGDRtWeivuX/7yFzp1qpGBik0F2reH\nuDjnLqQOHWr+/OIMPxQ5srOzdd68eW6HYY7BsmXLOO6449wOIyz4fD58Ph+JiYmsWrWKwYMHs2rV\nKmLDZW5GP/vMatbkyc4P2W6/vfoGyBOR71S10ptdw+svz5haZu/evZx++un4fD5UtfQbv6ndvF5Y\ntgw2bnSGv6hJ9tdnjIsaNmzId99953YYJsx06uR0Oq9YUfNJodZ0NBtjTKRIToY2bdwZIM+SgjHG\nhCGvF3JzYceOmj2vJQVjjAlDJQPk1XRtwZKCMcaEoYYNoVkzSwrGhFx1DJ0NMGHCBDZv3lzutq+/\n/prevXuTlZXFcccdx6OPPlpd4ZtaxOuFDRugJscptLuPTK0TzNDZwZgwYQI9e/akWbPDZ/C7+uqr\neeedd+jatStFRUWVzr0QjKKiIjxujHtgXOP1wowZsHIlHH98zZzTagrGBJg0aRK9evUiKyuLm266\nieLiYnw+H1deeSXdunWja9euPPPMM0yePJkFCxYwbNiwcmsYW7duLU0WHo+ndGTVvLw8rr766tJx\njN555x0AXnrppdLjl4xSWjIs92233Ub37t2ZM2cOc+fOpX///pxwwgkMHTqULVu21OC7Y2pa06ZO\nM1JNNiFZTcG46qOPoIIWmKPWrBkMGVL11y1evJi3336bWbNmERsby4gRI3jttdfo0KED27ZtY9Gi\nRQDs2rWLhg0bMmbMGP75z3+SlZV12LFuu+02OnXqxMCBAxk6dChXXXUVCQkJPPzww6SlpbFw4UJU\nlV27dpGTk8Mf//hH5s2bR4MGDRg0aBDvv/8+Q4YMYffu3Zx66qk89dRTHDhwgIEDBzJ16lRSU1N5\n+eWXeeCBBxg/fvyxvmUmTIk403R+9x0UFkLA2IshYzUFY/w+/fRT5s6dS3Z2NllZWXz55ZesWbOG\njh07smLFCkaNGsX06dMPG5uoPH/605+YO3cugwYN4sUXX+Tss88uPUfJjGwiQqNGjZg9ezannXYa\nqampxMXF8etf/7p0ALz4+HguvNCZFGrZsmUsWbKEQYMGkZWVxejRo9mwYUOI3g0TLrxeZyrhNWtq\n5nxWUzCuOppv9KGiqvzmN78pt1N44cKFfPjhh4wdO5Y333wzqG/nHTt2pGPHjlx//fWkpqaye/fu\nKseUlJRUOrKvqtK9e/fSuRFM7dC2LSQlOU1INTH8lNUUjPEbNGgQU6ZMYdu2bYBzl9JPP/3E1q1b\nUVUuvfRSHnnkEebPnw9AvXr1yMvLK/dYH3zwASWDTa5atYqEhATq1avHGWecwdixYwHnIr9z5056\n9+7NF198wfbt2/H5fLz22mv079//sGNmZmby888/M2fOHAAKCwtZsmRJtb8PJryUzLGwciUUF4f+\nfFZTMMavW7duPPTQQwwaNIji4mLi4uJ49tln8Xg8XHfddagqIsJf//pXAK699lquv/56kpKSmDNn\nziGT7UycOJE77riDpKQk4uLieOWVV4iJieGhhx7ipptuomvXrng8Hh599FHOO+88Hn30UQYMGICq\ncu6553L22WcfNhFPQkICb7zxBqNGjWLPnj0UFRVx5513Vjhbm4keGRnwww/w00+Qnh7ac9nQ2abG\n2TDMkcc+M3cVFsKYMTBoEPTocXTHsKGzjTEmSsTHwx13VN/cCkdifQrGGBMBamomYUsKxhWR1mxZ\nm9lnVbtYUjA1LjExke3bt9vFJgKoKtu3bycxMdHtUEwNsT4FU+NatWpFTk4OW7dudTsUE4TExERa\ntWrldhimhoQ0KYjIEOBpwAP8R1VHl9l+DfAE8LN/1T9V9T+hjMm4Ly4ujnbt2rkdhjGmHCFLCiLi\nAcYCZwA5wFwRmaqqS8vsOllVbwlVHMYYY4IXyj6FXsBqVV2rqoXAa8D5ITyfMcaYYxTKpNASCByt\nK8e/rqyLRWShiLwhIq3LO5CIjBCReSIyz9qhjTEmdNzuaH4PeFVVD4jIjcAk4LSyO6nqeGA8gIhs\nFZH1QAMgcISxwOWKtqUC26op9rLnONr9Ktpe3vpgyxz4vLrKHGx5g9nXylzx+qosR2KZq/oZl10O\n5zJX19912eXqKnPboPZS1ZA8gD7A9IDl+4D7jrC/B9hdheOPr2i5om3AvGos3/jq2K+i7eWtD7bM\nZZ5XS5mDLa+V+djKXJXlSCxzVT/jSCpzdf1d10SZj/QIZfPRXKCTiLQTkXjgMmBq4A4i0jxg8Txg\nWRWO/94Rlo+0rboEe8zK9qtoe3nrgy2zm+UNZl8rc8Xrq7IciWWu6mdcdjmcy1xdf9dll0NR5gqF\ndEA8ETkLeAqnFjBBVf8sIo/gZLupIvIYTjLwATuAkaoasonnRGSeBjEgVDSxMtcOVubaoSbKHNI+\nBVWdBkwrs+7BgOf34TQr1ZTaOG+hlbl2sDLXDiEvc8QNnW2MMSZ0bOwjY4wxpSwpGGOMKWVJwRhj\nTClLCgFEpI7/l9PnuB1LTRCR40TkWf+vyUe6HU9NEJELROQ5EZksIoPdjqcmiEh7EXleRN5wO5ZQ\n8f/fneT/bC93O56aEKrPNSqSgohMEJFcEVlcZv0QEVkhIqtF5N4gDvV7YEpooqxe1VFmVV2mqr8F\nfgX0DWW81aGayvyOqt4A/BYYFsp4q0M1lXmtql4X2kirXxXLfhHwhv+zPa/Gg60mVSlzqD7XqEgK\nwERgSOCKgFFahwKZwHARyRSRbiLyfplHExE5A1gK5NZ08EdpIsdYZv9rzgM+oMytw2FqItVQZr8/\n+l8X7iZSfWWONBMJsuxAK34Za62oBmOsbhMJvswh4fbYR9VCVb8SkfQyq0tHaQUQkdeA81X1MeCw\n5iERGQDUwXnT94nINFUtDmXcx6I6yuw/zlRgqoh8ALwSuoiPXTV9zgKMBj5U1fmhjfjYVdfnHImq\nUnacATdbAQuI4C+7VSxz2WkIqkXEvnlBCHaUVgBU9Q+qehvOhfG5cE4IR1ClMovIABF5RkTGERk1\nhfJUqczArcAg4BIR+W0oAwuhqn7OKSLyLHC8iNTkj0VDoaKyv4Uz4vK/qeFhIWpAuWUO1ecaFTWF\n6qSqE92Ooaao6gxghsth1ChVfQZ4xu04apKqbsfpQ4laqpoPXOt2HDUpVJ9rNNcUfgYC52doxS/T\nfkYrK7OVOdrVxrLXaJmjOSlUOkprFLIyW5mjXW0se42WOSqSgoi8CnwDZIhIjohcp6o+4BZgOs6Q\n3FNUdYmbcVYnK7OVmSgtc4naWPZwKLMNiGeMMaZUVNQUjDHGVA9LCsYYY0pZUjDGGFPKkoIxxphS\nlhSMMcaUsqRgjDGmlCUFY4wxpSwpGGOMKWVJwRhARJJE5Ev/2PVhSUTiReQrEbGBLE3IWFIwxvEb\n4C1VDdsJWlS1EPiMCJgxzkQuSwomqolIJxFZJyId/ctxIrJARFqX2fVy4F3/Pg/4pz6cKSKvishd\nAcd7R0S+E5ElIjLCvy5dRJaLyEQRWSkiL4vIIBH5WkRWiUivKu532DkCvOOP1ZiQsLGPTNQTkfuB\n3ao6VkRuBxqp6oMB2+OBn1S1mYicCDwHnATEAfOBcar6pH/fxqq6Q0SScEav7A/UA1YDxwNL/Ot/\nAK7DmS/4WlW9wD+jVjD7HXYO/9j5JVMzblbVtFC9X6Z2s7ZJUxssBgaJSGOcC3DvMttTgV3+532B\nd1V1P7BfRMrO4jVKRC70P28NdAI2Az+q6iIAEVkCfKaqKiKLgPSA1wezX3nn2A6gqkUiUigi9VQ1\n72jeDGOOxJqPTG2wEsgAHgae9M/SFWgfkFjZQfzzeA8C+qhqD+D7gNcdCNi1OGC5mEO/fB1xv0rO\nUSIB2F9ZvMYcDUsKpjZYA/TEmQD9xbIbVXUn4BGRROBr4FwRSRSRusA5Abs2AHaqaoGIeHGamKrb\nEc8hIinANlU9GIJzG2NJwUQ//wV0D3CvqhZXsNvHwCmqOhdnVquFwIfAImC3f5+PcL7NLwNGA9+G\nINzKzjEQ+CAE5zUGsI5mU0uIyE9AW63gD15EegK3q+qVIlJXVfeKSDLwFTBCVefXZLwVEZG3cJLb\nSrdjMdHJOppN1PPf9bO+ooQAoKrzReQL/90940UkE6ctf1IYJYR44B1LCCaUrKZgjDGmlPUpGGOM\nKWVJwRhjTClLCsYYY0pZUjDGGFPKkoIxxphSlhSMMcaUsqRgjDGmlCUFY4wxpf4fri2RfgBdWPYA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbacb6c9400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train, test = answer_six()\n",
    "# gamma_vals = np.logspace(-4,1,6)\n",
    "\n",
    "# plt.title('Validation Curve with SVM')\n",
    "# plt.xlabel('$\\gamma$ (gamma)')\n",
    "# plt.ylabel('Score')\n",
    "# #these generate the blank plot / figure, now will graph on top of it\n",
    "# # lw = linewidth\n",
    "# plt.semilogx(gamma_vals, train, label='Training Score',\n",
    "#              ls='-.',\n",
    "#              color='k')\n",
    "# plt.semilogx(gamma_vals, test, label='Test Score',\n",
    "#              ls='-',\n",
    "#              alpha = 0.5,\n",
    "#             color='b')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001, 1, 0.1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    train, test = answer_six()\n",
    "    Underfitting = .0001\n",
    "    Overfitting = 10 \n",
    "    Good_Generalization = .1\n",
    "    \n",
    "    return (Underfitting, Overfitting, Good_Generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "eWYHL",
   "launcher_item_id": "BAqef",
   "part_id": "fXXRp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
